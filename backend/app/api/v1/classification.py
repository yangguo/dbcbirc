from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from typing import List, Optional
import pandas as pd
import io
import os
import json
from datetime import datetime, timedelta
from pydantic import BaseModel
import openai
from app.core.config import settings
import asyncio

# Initialize OpenAI client
client = openai.OpenAI(
    api_key=settings.OPENAI_API_KEY,
    base_url=settings.OPENAI_BASE_URL,
    timeout=120.0,  # Increased timeout to 2 minutes
    max_retries=5   # Increased retries
)

def get_class(article, candidate_labels, multi_label=False):
    """ä½¿ç”¨OpenAI LLMå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»"""
    try:
        # åˆ›å»ºåˆ†ç±»æç¤ºè¯
        labels_str = ", ".join(candidate_labels)
        prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»åˆ°è¿™äº›ç±»åˆ«ä¸­çš„ä¸€ä¸ª: {labels_str}
        
è¦åˆ†ç±»çš„æ–‡æœ¬: {article}
        
è¯·åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹:
        {{
            "label": "é€‰æ‹©çš„ç±»åˆ«",
            "score": 0åˆ°1ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°
        }}
        
ä»æä¾›çš„ç±»åˆ«åˆ—è¡¨ä¸­é€‰æ‹©æœ€åˆé€‚çš„ç±»åˆ«ã€‚"""
        
        response = client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»åŠ©æ‰‹ã€‚è¯·å§‹ç»ˆè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=150
        )
        
        # è§£æå“åº”
        result_text = response.choices[0].message.content.strip()
        try:
            result = json.loads(result_text)
            return {
                "label": result.get("label", candidate_labels[0]),
                "score": float(result.get("score", 0.5))
            }
        except json.JSONDecodeError:
            # JSONè§£æå¤±è´¥æ—¶çš„å›é€€
            return {
                "label": candidate_labels[0],
                "score": 0.5
            }
            
    except openai.APIConnectionError as e:
        print(f"APIè¿æ¥å¤±è´¥: {str(e)}")
        return {
            "label": candidate_labels[0] if candidate_labels else "unknown",
            "score": 0.0
        }
    except openai.APITimeoutError as e:
        print(f"APIè¯·æ±‚è¶…æ—¶: {str(e)}")
        return {
            "label": candidate_labels[0] if candidate_labels else "unknown",
            "score": 0.0
        }
    except Exception as e:
        print(f"åˆ†ç±»å¤±è´¥: {str(e)}")
        return {
            "label": candidate_labels[0] if candidate_labels else "unknown",
            "score": 0.0
        }

router = APIRouter()


class ClassifyTextRequest(BaseModel):
    text: str
    labels: List[str]


class ClassifyTextLLMRequest(BaseModel):
    text: str
    candidate_labels: List[str]
    multi_label: Optional[bool] = False


class ClassificationResult(BaseModel):
    text: str
    predicted_labels: List[str]
    confidence: float


class LLMClassificationResult(BaseModel):
    text: str
    label: str
    score: float
    candidate_labels: List[str]


class ExtractInfoRequest(BaseModel):
    text: str


class ExtractInfoResult(BaseModel):
    success: bool
    data: Optional[List[dict]] = None
    error: Optional[str] = None


def extract_penalty_info_fallback(text):
    """åŸºæœ¬çš„æ–‡æœ¬è§£æä½œä¸ºLLMå¤±è´¥æ—¶çš„fallback"""
    try:
        import re
        
        # å°è¯•ä»æ–‡æœ¬ä¸­æå–åŸºæœ¬ä¿¡æ¯
        results = []
        
        # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        # æŸ¥æ‰¾å…¬å¸åç§°æ¨¡å¼
        company_pattern = r'([^0-9\s]+(?:ä¿é™©|é“¶è¡Œ|ä¿¡æ‰˜|è¯åˆ¸|åŸºé‡‘|é‡‘è)(?:è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸|å…¬å¸))'
        companies = re.findall(company_pattern, text)
        
        # æŸ¥æ‰¾é‡‘é¢æ¨¡å¼
        amount_pattern = r'(\d+(?:\.\d+)?ä¸‡å…ƒ)'
        amounts = re.findall(amount_pattern, text)
        
        # æŸ¥æ‰¾ç›‘ç®¡æœºå…³
        regulator_pattern = r'(é‡‘èç›‘ç®¡æ€»å±€|é“¶ä¿ç›‘[å±€ä¼š]|äººæ°‘é“¶è¡Œ)'
        regulators = re.findall(regulator_pattern, text)
        
        # å¦‚æœæ‰¾åˆ°äº†å…¬å¸ï¼Œä¸ºæ¯ä¸ªå…¬å¸åˆ›å»ºä¸€ä¸ªè®°å½•
        if companies:
            for i, company in enumerate(companies):
                amount_str = "0"
                if i < len(amounts):
                    # è½¬æ¢é‡‘é¢æ ¼å¼
                    amount_text = amounts[i]
                    if 'ä¸‡å…ƒ' in amount_text:
                        amount_num = float(amount_text.replace('ä¸‡å…ƒ', ''))
                        amount_str = str(int(amount_num * 10000))
                
                regulator = regulators[0] if regulators else "é‡‘èç›‘ç®¡æ€»å±€"
                
                results.append({
                    "è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·": "",
                    "è¢«å¤„ç½šå½“äº‹äºº": company,
                    "ä¸»è¦è¿æ³•è¿è§„äº‹å®": "åŸºæœ¬è§£ææ¨¡å¼ï¼Œè¯¦ç»†ä¿¡æ¯éœ€è¦LLMå¤„ç†",
                    "è¡Œæ”¿å¤„ç½šä¾æ®": "ã€Šä¿é™©æ³•ã€‹ç­‰ç›¸å…³è§„å®š",
                    "è¡Œæ”¿å¤„ç½šå†³å®š": "è¯¦ç»†å¤„ç½šå†…å®¹éœ€è¦LLMå¤„ç†",
                    "ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°": regulator,
                    "ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ": "",
                    "è¡Œä¸š": "ä¿é™©ä¸š" if "ä¿é™©" in company else "é‡‘èä¸š",
                    "ç½šæ²¡æ€»é‡‘é¢": amount_str,
                    "è¿è§„ç±»å‹": "éœ€è¦LLMåˆ†æ",
                    "ç›‘ç®¡åœ°åŒº": ""
                })
        
        if not results:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¿¡æ¯ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬ç»“æ„
            results = [{
                "è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·": "",
                "è¢«å¤„ç½šå½“äº‹äºº": "è§£æå¤±è´¥",
                "ä¸»è¦è¿æ³•è¿è§„äº‹å®": "åŸºæœ¬è§£ææ¨¡å¼æ— æ³•æå–è¯¦ç»†ä¿¡æ¯",
                "è¡Œæ”¿å¤„ç½šä¾æ®": "",
                "è¡Œæ”¿å¤„ç½šå†³å®š": "",
                "ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°": "",
                "ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ": "",
                "è¡Œä¸š": "",
                "ç½šæ²¡æ€»é‡‘é¢": "0",
                "è¿è§„ç±»å‹": "",
                "ç›‘ç®¡åœ°åŒº": ""
            }]
        
        return {
            "success": True,
            "data": results,
            "fallback_mode": True
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"åŸºæœ¬è§£æä¹Ÿå¤±è´¥: {str(e)}"
        }


def extract_penalty_info(text):
    """ä½¿ç”¨LLMæå–è¡Œæ”¿å¤„ç½šå†³å®šä¹¦å…³é”®ä¿¡æ¯"""
    try:
        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®
        if not settings.OPENAI_API_KEY:
            print("APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨fallbackæ¨¡å¼")
            return extract_penalty_info_fallback(text)
        
        # æ£€æŸ¥è¾“å…¥æ–‡æœ¬æ˜¯å¦ä¸ºç©º
        if not text or not text.strip():
            return {
                "success": False,
                "error": "è¾“å…¥æ–‡æœ¬ä¸ºç©º"
            }
        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ä¿¡æ¯æŠ½å–æ¨¡å‹ã€‚è¯·ä»è¾“å…¥çš„è¡¨æ ¼æ•°æ®ä¸­æå–è¡Œæ”¿å¤„ç½šä¿¡æ¯ã€‚

è¾“å…¥æ–‡æœ¬åŒ…å«è¡¨æ ¼æ ¼å¼çš„è¡Œæ”¿å¤„ç½šæ•°æ®ï¼Œå¯èƒ½åŒ…å«ä»¥ä¸‹åˆ—ï¼šåºå·ã€å½“äº‹äººåç§°ã€æœºæ„åœ°å€ã€ä¸»è¦è¿æ³•è¿è§„è¡Œä¸ºã€è¡Œæ”¿å¤„ç½šå†…å®¹ã€ä½œå‡ºå†³å®šæœºå…³ç­‰ã€‚

è¯·ä¸ºæ¯ä¸ªå¤„ç½šè®°å½•æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼š

[
  {{
    "è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·": "æ–‡å·ä¿¡æ¯ï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®æ–‡å·ï¼Œå¡«å†™ç©ºå­—ç¬¦ä¸²ï¼‰",
    "è¢«å¤„ç½šå½“äº‹äºº": "å½“äº‹äººåç§°",
    "ä¸»è¦è¿æ³•è¿è§„äº‹å®": "è¿æ³•è¿è§„è¡Œä¸ºæè¿°",
    "è¡Œæ”¿å¤„ç½šä¾æ®": "æ³•å¾‹ä¾æ®ï¼ˆå¦‚ã€Šä¿é™©æ³•ã€‹ç­‰ï¼‰",
    "è¡Œæ”¿å¤„ç½šå†³å®š": "å…·ä½“å¤„ç½šå†…å®¹",
    "ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°": "å†³å®šæœºå…³",
    "ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ": "æ—¥æœŸï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®æ—¥æœŸï¼Œå¡«å†™ç©ºå­—ç¬¦ä¸²ï¼‰",
    "è¡Œä¸š": "æ‰€å±è¡Œä¸šï¼ˆå¦‚ä¿é™©ä¸šã€é“¶è¡Œä¸šç­‰ï¼‰",
    "ç½šæ²¡æ€»é‡‘é¢": "æ•°å­—å½¢å¼çš„é‡‘é¢ï¼ˆå•ä½ï¼šå…ƒï¼Œå¦‚253ä¸‡å…ƒè½¬æ¢ä¸º2530000ï¼Œå¦‚æ— æ³•ç¡®å®šå¡«å†™0ï¼‰",
    "è¿è§„ç±»å‹": "è¿è§„ç±»å‹åˆ†ç±»",
    "ç›‘ç®¡åœ°åŒº": "ç›¸å…³åœ°åŒºæˆ–çœä»½"
  }}
]

å¤„ç†è¦æ±‚ï¼š
1. ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œè¯†åˆ«æ¯ä¸ªå®Œæ•´çš„å¤„ç½šè®°å½•
2. æ‰€æœ‰å­—æ®µå€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹
3. é‡‘é¢å­—æ®µéœ€è¦è½¬æ¢ä¸ºçº¯æ•°å­—å­—ç¬¦ä¸²ï¼ˆå¦‚"2530000"ï¼‰
4. åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—
5. ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«è§£æ

è¾“å…¥æ•°æ®ï¼š
{text}"""
        
        # Add retry logic with exponential backoff
        import time
        max_retries = 3
        base_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿”å›æœ‰æ•ˆçš„JSONæ•°ç»„æ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ã€è§£é‡Šæ–‡å­—æˆ–å…¶ä»–å†…å®¹ã€‚ç¡®ä¿è¿”å›çš„å†…å®¹å¯ä»¥ç›´æ¥è¢«json.loads()è§£æã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=2000,
                    timeout=90.0  # Per-request timeout
                )
                break  # Success, exit retry loop
            except (openai.APITimeoutError, openai.APIConnectionError) as e:
                if attempt == max_retries - 1:  # Last attempt
                    raise e
                delay = base_delay * (2 ** attempt)  # Exponential backoff
                print(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}), {delay}ç§’åé‡è¯•: {str(e)}")
                time.sleep(delay)
        
        # è§£æå“åº”
        result_text = response.choices[0].message.content.strip()
        
        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if result_text.startswith('```json'):
            result_text = result_text[7:]
        if result_text.startswith('```'):
            result_text = result_text[3:]
        if result_text.endswith('```'):
            result_text = result_text[:-3]
        result_text = result_text.strip()
        
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            result = json.loads(result_text)
            # ç¡®ä¿ç»“æœæ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(result, list):
                result = [result]  # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå•å…ƒç´ åˆ—è¡¨
            return {
                "success": True,
                "data": result
            }
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONæ•°ç»„éƒ¨åˆ†
            import re
            
            # é¦–å…ˆå°è¯•åŒ¹é…å®Œæ•´çš„JSONæ•°ç»„ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
            json_array_pattern = r'\[(?:[^[\]]*|\[[^\]]*\])*\]'
            json_array_match = re.search(json_array_pattern, result_text, re.DOTALL)
            if json_array_match:
                try:
                    result = json.loads(json_array_match.group())
                    if isinstance(result, list):
                        return {
                            "success": True,
                            "data": result
                        }
                except json.JSONDecodeError:
                    pass
            
            # å¦‚æœæ•°ç»„åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ¹é…å•ä¸ªJSONå¯¹è±¡
            json_object_pattern = r'\{(?:[^{}]*|\{[^}]*\})*\}'
            json_matches = re.findall(json_object_pattern, result_text, re.DOTALL)
            if json_matches:
                try:
                    results = []
                    for match in json_matches:
                        obj = json.loads(match)
                        results.append(obj)
                    return {
                        "success": True,
                        "data": results
                    }
                except json.JSONDecodeError:
                    pass
            
            # æœ€åå°è¯•ï¼šæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„JSONç‰‡æ®µå¹¶å°è¯•ä¿®å¤
            try:
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                fixed_text = result_text
                # ä¿®å¤å¯èƒ½çš„å°¾éšé€—å·
                fixed_text = re.sub(r',\s*}', '}', fixed_text)
                fixed_text = re.sub(r',\s*]', ']', fixed_text)
                
                result = json.loads(fixed_text)
                if not isinstance(result, list):
                    result = [result]
                return {
                    "success": True,
                    "data": result
                }
            except json.JSONDecodeError:
                pass
            
            # å¦‚æœæ‰€æœ‰JSONè§£æéƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœç»“æ„
            print(f"JSONè§£æå®Œå…¨å¤±è´¥ï¼ŒåŸå§‹å“åº”: {result_text[:200]}...")
            
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–åŸºæœ¬ä¿¡æ¯ä½œä¸ºfallback
            fallback_result = [{
                "è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·": "",
                "è¢«å¤„ç½šå½“äº‹äºº": "è§£æå¤±è´¥",
                "ä¸»è¦è¿æ³•è¿è§„äº‹å®": "LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æ",
                "è¡Œæ”¿å¤„ç½šä¾æ®": "",
                "è¡Œæ”¿å¤„ç½šå†³å®š": "",
                "ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°": "",
                "ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ": "",
                "è¡Œä¸š": "",
                "ç½šæ²¡æ€»é‡‘é¢": "0",
                "è¿è§„ç±»å‹": "",
                "ç›‘ç®¡åœ°åŒº": ""
            }]
            
            return {
                "success": False,
                "error": "æ— æ³•è§£æLLMè¿”å›çš„JSONæ ¼å¼",
                "data": fallback_result,
                "raw_response": result_text[:500] + "..." if len(result_text) > 500 else result_text
            }
            
    except openai.APIConnectionError as e:
        print(f"APIè¿æ¥å¤±è´¥è¯¦æƒ…: {str(e)}, ä½¿ç”¨fallbackæ¨¡å¼")
        fallback_result = extract_penalty_info_fallback(text)
        fallback_result["error"] = f"APIè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬è§£ææ¨¡å¼: {str(e)}"
        return fallback_result
    except openai.APITimeoutError as e:
        print(f"APIè¯·æ±‚è¶…æ—¶è¯¦æƒ…: {str(e)}, ä½¿ç”¨fallbackæ¨¡å¼")
        fallback_result = extract_penalty_info_fallback(text)
        fallback_result["error"] = f"APIè¯·æ±‚è¶…æ—¶ï¼Œä½¿ç”¨åŸºæœ¬è§£ææ¨¡å¼: {str(e)}"
        return fallback_result
    except openai.RateLimitError as e:
        print(f"APIé™æµè¯¦æƒ…: {str(e)}")
        return {
            "success": False,
            "error": f"APIé™æµ: {str(e)}. è¯·ç¨åé‡è¯•"
        }
    except openai.AuthenticationError as e:
        print(f"APIè®¤è¯å¤±è´¥è¯¦æƒ…: {str(e)}")
        return {
            "success": False,
            "error": f"APIè®¤è¯å¤±è´¥: {str(e)}. è¯·æ£€æŸ¥APIå¯†é’¥"
        }
    except Exception as e:
        print(f"LLMåˆ†æå¤±è´¥è¯¦æƒ…: {str(e)}")
        print(f"è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text) if text else 0}")
        return {
             "success": False,
             "error": f"LLMåˆ†æå¤±è´¥: {str(e)}"
         }


@router.post("/extract-penalty-info", response_model=ExtractInfoResult)
async def extract_penalty_info_endpoint(request: ExtractInfoRequest):
    """æå–è¡Œæ”¿å¤„ç½šå†³å®šä¹¦å…³é”®ä¿¡æ¯"""
    try:
        result = extract_penalty_info(request.text)
        return ExtractInfoResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¿¡æ¯æå–å¤±è´¥: {str(e)}")


@router.post("/classify-text-llm")
async def classify_text_llm(request: ClassifyTextLLMRequest):
    """ä½¿ç”¨LLMå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œè¾“å…¥æ–‡æœ¬å’Œå€™é€‰æ ‡ç­¾ï¼Œè¾“å‡ºJSONæ ¼å¼çš„åˆ†ç±»ç»“æœ"""
    try:
        # è°ƒç”¨classifier.pyä¸­çš„get_classå‡½æ•°
        result = get_class(
            article=request.text,
            candidate_labels=request.candidate_labels,
            multi_label=request.multi_label
        )
        
        return {
            "text": request.text,
            "label": result["label"],
            "score": result["score"],
            "candidate_labels": request.candidate_labels,
            "multi_label": request.multi_label
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†ç±»å¤±è´¥: {str(e)}")


@router.post("/classify-text")
async def classify_text(request: ClassifyTextRequest):
    """Classify a single text input"""
    try:
        # This is a placeholder implementation
        # In a real scenario, you would use an AI model for classification
        
        # Simple keyword-based classification for demonstration
        text_lower = request.text.lower()
        predicted_labels = []
        confidence = 0.0
        
        # Basic keyword matching
        keyword_mapping = {
            "è¿è§„æ”¾è´·": ["æ”¾è´·", "è´·æ¬¾", "ä¿¡è´·"],
            "å†…æ§ç®¡ç†": ["å†…æ§", "ç®¡ç†", "åˆ¶åº¦"],
            "åæ´—é’±": ["æ´—é’±", "åæ´—é’±", "å¯ç–‘äº¤æ˜“"],
            "æ¶ˆè´¹è€…æƒç›Š": ["æ¶ˆè´¹è€…", "æƒç›Š", "æŠ•è¯‰"],
            "ä¿¡æ¯æŠ«éœ²": ["æŠ«éœ²", "ä¿¡æ¯", "å…¬å¼€"],
            "é£é™©ç®¡ç†": ["é£é™©", "ç®¡ç†", "æ§åˆ¶"]
        }
        
        for label in request.labels:
            if label in keyword_mapping:
                keywords = keyword_mapping[label]
                if any(keyword in text_lower for keyword in keywords):
                    predicted_labels.append(label)
                    confidence += 0.8
        
        if not predicted_labels:
            # Default to first label if no matches
            predicted_labels = [request.labels[0]] if request.labels else ["å…¶ä»–"]
            confidence = 0.3
        
        confidence = min(confidence / len(predicted_labels), 1.0) if predicted_labels else 0.0
        
        return {
            "text": request.text,
            "predicted_labels": predicted_labels,
            "confidence": round(confidence, 2)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class SaveSuccessfulRecordsRequest(BaseModel):
    results: List[dict]


@router.post("/save-successful-records")
async def save_successful_records(request: SaveSuccessfulRecordsRequest):
    """ä¿å­˜æˆåŠŸçš„è®°å½•åˆ°ä¸¤ä¸ªCSVæ–‡ä»¶"""
    try:
        # è¿‡æ»¤å‡ºæˆåŠŸçš„è®°å½•
        successful_records = [record for record in request.results if record.get("çŠ¶æ€") == "æˆåŠŸ"]
        
        if not successful_records:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æˆåŠŸçš„è®°å½•å¯ä»¥ä¿å­˜")
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„cbircæ–‡ä»¶å¤¹è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
        cbirc_dir = os.path.join(project_root, "cbirc")
        
        # ç¡®ä¿cbircç›®å½•å­˜åœ¨
        os.makedirs(cbirc_dir, exist_ok=True)
        
        # å‡†å¤‡ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æ•°æ® (cbircsplit)
        split_data = []
        for record in successful_records:
            split_record = {
                "wenhao": record.get("è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·", ""),
                "people": record.get("è¢«å¤„ç½šå½“äº‹äºº", ""),
                "event": record.get("ä¸»è¦è¿æ³•è¿è§„äº‹å®", ""),
                "law": record.get("è¡Œæ”¿å¤„ç½šä¾æ®", ""),
                "penalty": record.get("è¡Œæ”¿å¤„ç½šå†³å®š", ""),
                "org": record.get("ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°", ""),
                "date": record.get("ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ", ""),
                "id": record.get("åŸå§‹ID", "")
            }
            split_data.append(split_record)
        
        # å‡†å¤‡ç¬¬äºŒä¸ªæ–‡ä»¶çš„æ•°æ® (cbirccat)
        cat_data = []
        for record in successful_records:
            cat_record = {
                "id": record.get("åŸå§‹ID", ""),
                "amount": record.get("ç½šæ²¡æ€»é‡‘é¢", ""),
                "industry": record.get("è¡Œä¸š", ""),
                "category": record.get("è¿è§„ç±»å‹", ""),
                "province": record.get("ç›‘ç®¡åœ°åŒº", "")
            }
            cat_data.append(cat_record)
        
        # ä¿å­˜ç¬¬ä¸€ä¸ªæ–‡ä»¶
        split_filename = f"cbircsplit{timestamp}.csv"
        split_filepath = os.path.join(cbirc_dir, split_filename)
        split_df = pd.DataFrame(split_data)
        split_df.to_csv(split_filepath, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜ç¬¬äºŒä¸ªæ–‡ä»¶
        cat_filename = f"cbirccat{timestamp}.csv"
        cat_filepath = os.path.join(cbirc_dir, cat_filename)
        cat_df = pd.DataFrame(cat_data)
        cat_df.to_csv(cat_filepath, index=False, encoding='utf-8-sig')
        
        return {
            "message": "æˆåŠŸè®°å½•å·²ä¿å­˜",
            "successful_count": len(successful_records),
            "files_created": [
                {
                    "filename": split_filename,
                    "filepath": split_filepath,
                    "records": len(split_data)
                },
                {
                    "filename": cat_filename,
                    "filepath": cat_filepath,
                    "records": len(cat_data)
                }
            ]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.post("/batch-extract-penalty-info")
async def batch_extract_penalty_info(
    file: UploadFile = File(...),
    id_column: str = Form("id"),
    content_column: str = Form("content")
):
    """æ‰¹é‡å¤„ç½šä¿¡æ¯æå–"""
    import time
    from datetime import datetime, timedelta
    
    start_time = time.time()
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒCSVå’ŒExcelæ–‡ä»¶")
        
        # è¯»å–æ–‡ä»¶
        file_read_start = time.time()
        contents = await file.read()
        file_size_mb = len(contents) / (1024 * 1024)
        
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        else:
            df = pd.read_excel(io.BytesIO(contents))
        
        file_read_time = time.time() - file_read_start
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        if id_column not in df.columns:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ—: {id_column}")
        if content_column not in df.columns:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ—: {content_column}")
        
        # åˆå§‹åŒ–æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
        results = []
        processing_logs = []
        success_count = 0
        failure_count = 0
        total_extracted_records = 0
        
        # ä¼°ç®—å®Œæˆæ—¶é—´ï¼ˆåŸºäºå†å²å¹³å‡å¤„ç†æ—¶é—´ï¼Œå‡è®¾æ¯æ¡è®°å½•å¹³å‡3ç§’ï¼‰
        estimated_time_per_record = 3.0
        estimated_total_time = len(df) * estimated_time_per_record
        estimated_completion = datetime.now() + timedelta(seconds=estimated_total_time)
        
        # è®°å½•å¼€å§‹ä¿¡æ¯
        start_log = f"[{current_time}] å¼€å§‹æ‰¹é‡å¤„ç½šä¿¡æ¯æå–ä»»åŠ¡"
        file_info_log = f"æ–‡ä»¶ä¿¡æ¯: {file.filename} (å¤§å°: {file_size_mb:.2f}MB, è¯»å–è€—æ—¶: {file_read_time:.2f}ç§’)"
        task_info_log = f"ä»»åŠ¡ä¿¡æ¯: æ€»è®°å½•æ•°={len(df)}, IDåˆ—='{id_column}', å†…å®¹åˆ—='{content_column}'"
        estimate_log = f"é¢„ä¼°å®Œæˆæ—¶é—´: {estimated_completion.strftime('%Y-%m-%d %H:%M:%S')} (é¢„è®¡è€—æ—¶: {estimated_total_time/60:.1f}åˆ†é’Ÿ)"
        
        print(start_log)
        print(file_info_log)
        print(task_info_log)
        print(estimate_log)
        
        processing_logs.extend([start_log, file_info_log, task_info_log, estimate_log])
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½•
        # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œè€Œä¸æ˜¯ç›¸å¯¹äºbackendç›®å½•çš„è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
        temp_dir = os.path.join(project_root, settings.DATA_FOLDER, "temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        # æ‰¹é‡æå–å¤„ç½šä¿¡æ¯
        for index, row in df.iterrows():
            record_start_time = time.time()
            record_id = str(row[id_column])
            text = str(row[content_column])
            text_length = len(text)
            
            # è®¡ç®—è¿›åº¦å’Œå‰©ä½™æ—¶é—´
            progress_percent = ((index + 1) / len(df)) * 100
            if index > 0:
                avg_time_per_record = (time.time() - start_time) / (index + 1)
                remaining_records = len(df) - (index + 1)
                estimated_remaining_time = remaining_records * avg_time_per_record
                estimated_finish = datetime.now() + timedelta(seconds=estimated_remaining_time)
                time_info = f"é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ, é¢„è®¡å®Œæˆ: {estimated_finish.strftime('%H:%M:%S')}"
            else:
                time_info = "è®¡ç®—å‰©ä½™æ—¶é—´ä¸­..."
            
            progress_log = f"[{datetime.now().strftime('%H:%M:%S')}] å¤„ç†è¿›åº¦: {index + 1}/{len(df)} ({progress_percent:.1f}%) - ID: {record_id}"
            input_log = f"è¾“å…¥ä¿¡æ¯: æ–‡æœ¬é•¿åº¦={text_length}å­—ç¬¦, {time_info}"
            
            print(progress_log)
            print(input_log)
            processing_logs.append(progress_log)
            processing_logs.append(input_log)
            
            # è®°å½•è¾“å…¥æ–‡æœ¬çš„å‰100ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
            if text_length > 0:
                text_preview = text[:100] + "..." if len(text) > 100 else text
                preview_log = f"è¾“å…¥æ–‡æœ¬é¢„è§ˆ: {text_preview}"
                processing_logs.append(preview_log)
            
            # è°ƒç”¨å¤„ç½šä¿¡æ¯æå–å‡½æ•°
            extract_result = extract_penalty_info(text)
            record_process_time = time.time() - record_start_time
            
            if extract_result["success"] and extract_result["data"]:
                # ä¸ºæ¯ä¸ªæå–çš„è®°å½•æ·»åŠ åŸå§‹ID
                extracted_count = len(extract_result["data"])
                success_count += 1
                total_extracted_records += extracted_count
                
                success_log = f"âœ“ è®°å½• {record_id} æå–æˆåŠŸ: {extracted_count}æ¡å¤„ç½šä¿¡æ¯ (è€—æ—¶: {record_process_time:.2f}ç§’)"
                print(success_log)
                processing_logs.append(success_log)
                
                # è®°å½•æå–åˆ°çš„å…³é”®ä¿¡æ¯
                for i, penalty_record in enumerate(extract_result["data"]):
                    entity_name = penalty_record.get("è¢«å¤„ç½šå½“äº‹äºº", "æœªçŸ¥")
                    amount = penalty_record.get("ç½šæ²¡æ€»é‡‘é¢", "0")
                    detail_log = f"  ç¬¬{i+1}æ¡: å½“äº‹äºº={entity_name}, ç½šæ¬¾é‡‘é¢={amount}å…ƒ"
                    processing_logs.append(detail_log)
                    
                    result_row = {
                        "åŸå§‹ID": record_id,
                        "çŠ¶æ€": "æˆåŠŸ",
                        "å¤„ç†æ—¶é—´": f"{record_process_time:.2f}ç§’",
                        **penalty_record
                    }
                    results.append(result_row)
            else:
                # å¦‚æœæå–å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé”™è¯¯è®°å½•
                error_msg = extract_result.get("error", "æœªçŸ¥é”™è¯¯")
                failure_count += 1
                
                failure_log = f"âœ— è®°å½• {record_id} æå–å¤±è´¥: {error_msg} (è€—æ—¶: {record_process_time:.2f}ç§’)"
                print(failure_log)
                processing_logs.append(failure_log)
                
                # åˆ›å»ºä¸æˆåŠŸæå–ä¸€è‡´çš„é”™è¯¯è®°å½•ç»“æ„
                error_record = {
                    "åŸå§‹ID": record_id,
                    "çŠ¶æ€": "å¤±è´¥",
                    "å¤„ç†æ—¶é—´": f"{record_process_time:.2f}ç§’",
                    "é”™è¯¯ä¿¡æ¯": error_msg,
                    "è¡Œæ”¿å¤„ç½šå†³å®šä¹¦æ–‡å·": "",
                    "è¢«å¤„ç½šå½“äº‹äºº": "",
                    "ä¸»è¦è¿æ³•è¿è§„äº‹å®": "",
                    "è¡Œæ”¿å¤„ç½šä¾æ®": "",
                    "è¡Œæ”¿å¤„ç½šå†³å®š": "",
                    "ä½œå‡ºå¤„ç½šå†³å®šçš„æœºå…³åç§°": "",
                    "ä½œå‡ºå¤„ç½šå†³å®šçš„æ—¥æœŸ": "",
                    "è¡Œä¸š": "",
                    "ç½šæ²¡æ€»é‡‘é¢": "",
                    "è¿è§„ç±»å‹": "",
                    "ç›‘ç®¡åœ°åŒº": ""
                }
                results.append(error_record)
            
            # æ¯å¤„ç†5æ¡è®°å½•ä¿å­˜ä¸€æ¬¡ä¸´æ—¶ç»“æœæ–‡ä»¶
            if (index + 1) % 5 == 0 or (index + 1) == len(df):
                temp_filename = f"temp_batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}_records_{index + 1}.csv"
                temp_filepath = os.path.join(temp_dir, temp_filename)
                
                try:
                    # å°†ç»“æœè½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜ä¸ºCSV
                    if results:
                        temp_df = pd.DataFrame(results)
                        temp_df.to_csv(temp_filepath, index=False, encoding='utf-8-sig')
                        
                        # åˆ›å»ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯æ–‡ä»¶
                        stats_filename = f"temp_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}_records_{index + 1}.txt"
                        stats_filepath = os.path.join(temp_dir, stats_filename)
                        
                        stats_content = f"""æ‰¹é‡å¤„ç½šä¿¡æ¯æå– - ä¸´æ—¶ç»Ÿè®¡æŠ¥å‘Š
æ—¶é—´æˆ³: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
å·²å¤„ç†è®°å½•æ•°: {index + 1}
æ€»è®°å½•æ•°: {len(df)}
å¤„ç†è¿›åº¦: {round(progress_percent, 2)}%
æˆåŠŸæå–: {success_count}
æå–å¤±è´¥: {failure_count}
æˆåŠŸç‡: {round((success_count / (index + 1)) * 100, 2) if (index + 1) > 0 else 0}%
æ€»æå–å¤„ç½šè®°å½•æ•°: {total_extracted_records}
å¤„ç†æ—¶é—´: {round((time.time() - start_time) / 60, 2)}åˆ†é’Ÿ
ç»“æœæ–‡ä»¶: {temp_filename}
"""
                        
                        with open(stats_filepath, 'w', encoding='utf-8') as f:
                            f.write(stats_content)
                        
                        temp_save_log = f"ğŸ’¾ ä¸´æ—¶ç»“æœå·²ä¿å­˜: {temp_filename} å’Œç»Ÿè®¡æ–‡ä»¶ {stats_filename} (å·²å¤„ç† {index + 1}/{len(df)} æ¡è®°å½•)"
                    else:
                        temp_save_log = f"ğŸ’¾ æš‚æ— ç»“æœæ•°æ®ï¼Œè·³è¿‡ä¸´æ—¶æ–‡ä»¶ä¿å­˜ (å·²å¤„ç† {index + 1}/{len(df)} æ¡è®°å½•)"
                    
                    print(temp_save_log)
                    processing_logs.append(temp_save_log)
                    
                except Exception as e:
                    temp_error_log = f"âš ï¸ ä¸´æ—¶æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}"
                    print(temp_error_log)
                    processing_logs.append(temp_error_log)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        avg_time_per_record = total_time / len(df)
        success_rate = (success_count / len(df)) * 100 if len(df) > 0 else 0
        
        completion_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_log = f"[{completion_time}] æ‰¹é‡æå–ä»»åŠ¡å®Œæˆ!"
        stats_log = f"å¤„ç†ç»Ÿè®¡: æ€»è®°å½•æ•°={len(df)}, æˆåŠŸ={success_count}, å¤±è´¥={failure_count}, æˆåŠŸç‡={success_rate:.1f}%"
        time_stats_log = f"æ—¶é—´ç»Ÿè®¡: æ€»è€—æ—¶={total_time/60:.2f}åˆ†é’Ÿ, å¹³å‡æ¯æ¡={avg_time_per_record:.2f}ç§’"
        output_log = f"è¾“å‡ºç»“æœ: æå–åˆ°{total_extracted_records}æ¡å¤„ç½šä¿¡æ¯, æ€»è¾“å‡ºè®°å½•æ•°={len(results)}"
        
        print(summary_log)
        print(stats_log)
        print(time_stats_log)
        print(output_log)
        
        processing_logs.extend([summary_log, stats_log, time_stats_log, output_log])
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œä¿ç•™æœ€åä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä½œä¸ºå¤‡ä»½ï¼‰
        try:
            temp_csv_files = [f for f in os.listdir(temp_dir) if f.startswith("temp_batch_results_") and f.endswith(".csv")]
            temp_stats_files = [f for f in os.listdir(temp_dir) if f.startswith("temp_stats_") and f.endswith(".txt")]
            
            total_cleaned = 0
            
            # æ¸…ç†CSVæ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„
            if len(temp_csv_files) > 1:
                temp_csv_files.sort()
                for temp_file in temp_csv_files[:-1]:
                    os.remove(os.path.join(temp_dir, temp_file))
                    total_cleaned += 1
            
            # æ¸…ç†ç»Ÿè®¡æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„
            if len(temp_stats_files) > 1:
                temp_stats_files.sort()
                for temp_file in temp_stats_files[:-1]:
                    os.remove(os.path.join(temp_dir, temp_file))
                    total_cleaned += 1
            
            if total_cleaned > 0:
                latest_csv = temp_csv_files[-1] if temp_csv_files else "æ— "
                latest_stats = temp_stats_files[-1] if temp_stats_files else "æ— "
                cleanup_log = f"ğŸ§¹ å·²æ¸…ç† {total_cleaned} ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°å¤‡ä»½: CSV={latest_csv}, ç»Ÿè®¡={latest_stats}"
                processing_logs.append(cleanup_log)
                print(cleanup_log)
        except Exception as e:
            cleanup_error_log = f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {str(e)}"
            processing_logs.append(cleanup_error_log)
            print(cleanup_error_log)
        
        return {
            "message": "æ‰¹é‡å¤„ç½šä¿¡æ¯æå–å®Œæˆ",
            "processed_count": len(df),
            "extracted_count": len(results),
            "success_count": success_count,
            "failure_count": failure_count,
            "success_rate": round(success_rate, 2),
            "total_penalty_records": total_extracted_records,
            "processing_time_minutes": round(total_time / 60, 2),
            "average_time_per_record": round(avg_time_per_record, 2),
            "file_size_mb": round(file_size_mb, 2),
            "completion_time": completion_time,
            "temp_files_saved": True,
            "temp_directory": temp_dir,
            "results": results,
            "processing_logs": processing_logs
        }
        
    except Exception as e:
        error_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        error_log = f"[{error_time}] æ‰¹é‡æå–ä»»åŠ¡å¤±è´¥: {str(e)}"
        print(error_log)
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æå–å¤±è´¥: {str(e)}")


@router.post("/batch-extract-penalty-info-stream")
async def batch_extract_penalty_info_stream(
    file: UploadFile = File(...),
    id_column: str = Form("id"),
    content_column: str = Form("content")
):
    """æ‰¹é‡å¤„ç½šä¿¡æ¯æå– - æµå¼å“åº”ç‰ˆæœ¬"""
    import time
    from datetime import datetime, timedelta
    
    # åœ¨ç”Ÿæˆå™¨å¤–éƒ¨è¯»å–æ–‡ä»¶ï¼Œé¿å…æ–‡ä»¶å…³é—­é—®é¢˜
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
            async def error_generator():
                yield f"data: {{\"type\": \"error\", \"message\": \"åªæ”¯æŒCSVå’ŒExcelæ–‡ä»¶\"}}\n\n"
            return StreamingResponse(
                error_generator(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_read_start = time.time()
        contents = await file.read()
        file_size_mb = len(contents) / (1024 * 1024)
        
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        else:
            df = pd.read_excel(io.BytesIO(contents))
        
        file_read_time = time.time() - file_read_start
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        if id_column not in df.columns:
            async def error_generator():
                yield f"data: {{\"type\": \"error\", \"message\": \"æ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ—: {id_column}\"}}\n\n"
            return StreamingResponse(
                error_generator(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )
        if content_column not in df.columns:
            async def error_generator():
                yield f"data: {{\"type\": \"error\", \"message\": \"æ–‡ä»¶ä¸­ä¸å­˜åœ¨åˆ—: {content_column}\"}}\n\n"
            return StreamingResponse(
                error_generator(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/event-stream"
                }
            )
    except Exception as e:
        async def error_generator():
            yield f"data: {{\"type\": \"error\", \"message\": \"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}\"}}\n\n"
        return StreamingResponse(
            error_generator(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
    
    async def generate_progress():
        start_time = time.time()
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        try:
            
            # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
            results = []
            success_count = 0
            failure_count = 0
            total_extracted_records = 0
            
            # å‘é€åˆå§‹ä¿¡æ¯
            start_info = {
                "type": "start",
                "message": f"[{current_time}] å¼€å§‹æ‰¹é‡å¤„ç½šä¿¡æ¯æå–ä»»åŠ¡",
                "file_info": f"æ–‡ä»¶ä¿¡æ¯: {file.filename} (å¤§å°: {file_size_mb:.2f}MB, è¯»å–è€—æ—¶: {file_read_time:.2f}ç§’)",
                "task_info": f"ä»»åŠ¡ä¿¡æ¯: æ€»è®°å½•æ•°={len(df)}, IDåˆ—='{id_column}', å†…å®¹åˆ—='{content_column}'",
                "total_records": len(df)
            }
            yield f"data: {json.dumps(start_info, ensure_ascii=False)}\n\n"
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç›®å½• - ä¿®æ­£è·¯å¾„è®¡ç®—
            # ä» backend/app/api/v1/classification.py åˆ°é¡¹ç›®æ ¹ç›®å½• (å‘ä¸Š5çº§)
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))
            temp_dir = os.path.join(project_root, settings.DATA_FOLDER, "temp")
            os.makedirs(temp_dir, exist_ok=True)
            
            # åå°æ—¥å¿—è®°å½•
            import logging
            logger = logging.getLogger(__name__)
            logger.info(f"æ‰¹é‡å¤„ç½šä¿¡æ¯æå–ä»»åŠ¡å¼€å§‹ - æ–‡ä»¶: {file.filename}, æ€»è®°å½•æ•°: {len(df)}, ä¸´æ—¶ç›®å½•: {temp_dir}")
            
            # é€æ¡å¤„ç†è®°å½•
            for index, row in df.iterrows():
                record_start_time = time.time()
                record_id = row[id_column]
                text = str(row[content_column])
                
                # å‘é€ç®€åŒ–çš„è¿›åº¦ä¿¡æ¯
                progress_info = {
                    "type": "progress",
                    "current": index + 1,
                    "total": len(df),
                    "percentage": round(((index + 1) / len(df)) * 100, 1),
                    "record_id": record_id
                }
                yield f"data: {json.dumps(progress_info, ensure_ascii=False)}\n\n"
                
                # è°ƒç”¨å¤„ç½šä¿¡æ¯æå–å‡½æ•°
                logger.info(f"å¼€å§‹å¤„ç†è®°å½• {record_id} ({index + 1}/{len(df)}) - æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                logger.debug(f"è®°å½• {record_id} æ–‡æœ¬é¢„è§ˆ: {text[:200]}..." if len(text) > 200 else f"è®°å½• {record_id} å®Œæ•´æ–‡æœ¬: {text}")
                extract_result = extract_penalty_info(text)
                record_process_time = time.time() - record_start_time
                
                if extract_result["success"] and extract_result["data"]:
                    extracted_count = len(extract_result["data"])
                    success_count += 1
                    total_extracted_records += extracted_count
                    
                    # åå°è¯¦ç»†æ—¥å¿—è®°å½•æˆåŠŸä¿¡æ¯
                    logger.info(f"âœ“ è®°å½• {record_id} æå–æˆåŠŸ: {extracted_count}æ¡å¤„ç½šä¿¡æ¯ (è€—æ—¶: {record_process_time:.2f}ç§’)")
                    logger.debug(f"è®°å½• {record_id} æå–çš„å¤„ç½šä¿¡æ¯è¯¦æƒ…: {json.dumps(extract_result['data'], ensure_ascii=False, indent=2)}")
                    logger.info(f"å½“å‰ç´¯è®¡ç»Ÿè®¡ - æˆåŠŸ: {success_count + 1}, å¤±è´¥: {failure_count}, æ€»æå–è®°å½•æ•°: {total_extracted_records + extracted_count}")
                    
                    # å‘é€ç®€åŒ–çš„æˆåŠŸä¿¡æ¯
                    success_info = {
                        "type": "success",
                        "record_id": record_id,
                        "extracted_count": extracted_count
                    }
                    yield f"data: {json.dumps(success_info, ensure_ascii=False)}\n\n"
                    
                    # æ·»åŠ åˆ°ç»“æœä¸­
                    for penalty_record in extract_result["data"]:
                        result_row = {
                            "åŸå§‹ID": record_id,
                            "çŠ¶æ€": "æˆåŠŸ",
                            "å¤„ç†æ—¶é—´": f"{record_process_time:.2f}ç§’",
                            **penalty_record
                        }
                        results.append(result_row)
                else:
                    failure_count += 1
                    error_msg = extract_result.get("error", "æœªçŸ¥é”™è¯¯")
                    
                    # åå°è¯¦ç»†æ—¥å¿—è®°å½•å¤±è´¥ä¿¡æ¯
                    logger.warning(f"âœ— è®°å½• {record_id} æå–å¤±è´¥: {error_msg} (è€—æ—¶: {record_process_time:.2f}ç§’)")
                    logger.error(f"è®°å½• {record_id} é”™è¯¯è¯¦æƒ…: {extract_result}")
                    logger.info(f"å½“å‰ç´¯è®¡ç»Ÿè®¡ - æˆåŠŸ: {success_count}, å¤±è´¥: {failure_count + 1}")
                    
                    # å‘é€ç®€åŒ–çš„å¤±è´¥ä¿¡æ¯
                    failure_info = {
                        "type": "failure",
                        "record_id": record_id
                    }
                    yield f"data: {json.dumps(failure_info, ensure_ascii=False)}\n\n"
                    
                    # æ·»åŠ å¤±è´¥è®°å½•
                    result_row = {
                        "åŸå§‹ID": record_id,
                        "çŠ¶æ€": "å¤±è´¥",
                        "é”™è¯¯ä¿¡æ¯": error_msg,
                        "å¤„ç†æ—¶é—´": f"{record_process_time:.2f}ç§’"
                    }
                    results.append(result_row)
                
                # æ¯5æ¡è®°å½•ä¿å­˜ä¸€æ¬¡ä¸´æ—¶æ–‡ä»¶
                if (index + 1) % 5 == 0 or (index + 1) == len(df):
                    try:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        temp_csv_path = os.path.join(temp_dir, f"temp_batch_results_{timestamp}_records_{index + 1}.csv")
                        temp_stats_path = os.path.join(temp_dir, f"temp_stats_{timestamp}_records_{index + 1}.txt")
                        
                        # åå°è¯¦ç»†æ—¥å¿—è®°å½•ä¸´æ—¶æ–‡ä»¶ä¿å­˜
                        logger.info(f"è§¦å‘ä¸´æ—¶æ–‡ä»¶ä¿å­˜ - å·²å¤„ç† {index + 1}/{len(df)} æ¡è®°å½• (æ¯5æ¡ä¿å­˜ä¸€æ¬¡)")
                        
                        # ä¿å­˜CSVç»“æœ
                        if results:
                            results_df = pd.DataFrame(results)
                            results_df.to_csv(temp_csv_path, index=False, encoding='utf-8-sig')
                            file_size_kb = os.path.getsize(temp_csv_path) / 1024
                            logger.info(f"ä¸´æ—¶CSVæ–‡ä»¶å·²ä¿å­˜: {temp_csv_path} ({len(results)}æ¡è®°å½•, {file_size_kb:.2f} KB)")
                        
                        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
                        current_time_elapsed = time.time() - start_time
                        current_success_rate = (success_count / (index + 1)) * 100 if (index + 1) > 0 else 0
                        logger.info(f"å½“å‰å¤„ç†ç»Ÿè®¡: æˆåŠŸç‡ {current_success_rate:.1f}%, å·²ç”¨æ—¶ {current_time_elapsed/60:.2f}åˆ†é’Ÿ, å¹³å‡æ¯æ¡ {current_time_elapsed/(index + 1):.2f}ç§’")
                        
                        stats_content = f"""æ‰¹é‡å¤„ç½šä¿¡æ¯æå– - ä¸´æ—¶ç»Ÿè®¡æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ–‡ä»¶å: {file.filename}

å¤„ç†è¿›åº¦:
- å·²å¤„ç†è®°å½•æ•°: {index + 1}/{len(df)}
- å¤„ç†è¿›åº¦: {((index + 1) / len(df)) * 100:.1f}%
- æˆåŠŸæå–: {success_count} æ¡
- æå–å¤±è´¥: {failure_count} æ¡
- æˆåŠŸç‡: {current_success_rate:.1f}%
- æ€»æå–å¤„ç½šä¿¡æ¯: {total_extracted_records} æ¡

æ—¶é—´ç»Ÿè®¡:
- å·²ç”¨æ—¶é—´: {current_time_elapsed/60:.2f} åˆ†é’Ÿ
- å¹³å‡æ¯æ¡: {current_time_elapsed/(index + 1):.2f} ç§’
"""
                        
                        with open(temp_stats_path, 'w', encoding='utf-8') as f:
                            f.write(stats_content)
                        stats_file_size_kb = os.path.getsize(temp_stats_path) / 1024
                        logger.info(f"ä¸´æ—¶ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜: {temp_stats_path} ({stats_file_size_kb:.2f} KB)")
                        
                        # å‘é€ç®€åŒ–çš„ä¸´æ—¶ä¿å­˜ä¿¡æ¯
                        temp_save_info = {
                            "type": "temp_save",
                            "processed_count": index + 1,
                            "total_count": len(df)
                        }
                        yield f"data: {json.dumps(temp_save_info, ensure_ascii=False)}\n\n"
                        
                    except Exception as e:
                        error_info = {
                            "type": "temp_save_error",
                            "message": f"âš ï¸ ä¸´æ—¶æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}"
                        }
                        yield f"data: {json.dumps(error_info, ensure_ascii=False)}\n\n"
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            total_time = time.time() - start_time
            avg_time_per_record = total_time / len(df)
            success_rate = (success_count / len(df)) * 100 if len(df) > 0 else 0
            
            # è®°å½•è¯¦ç»†çš„ä»»åŠ¡å®Œæˆæ—¥å¿—
            logger.info(f"="*80)
            logger.info(f"æ‰¹é‡å¤„ç½šä¿¡æ¯æå–ä»»åŠ¡å®Œæˆ")
            logger.info(f"æ–‡ä»¶ä¿¡æ¯: {file.filename} (å¤§å°: {file_size_mb:.2f} MB)")
            logger.info(f"å¤„ç†ç»Ÿè®¡: æ€»è®°å½• {len(df)}, æˆåŠŸ {success_count}, å¤±è´¥ {failure_count}")
            logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
            logger.info(f"æå–ç»“æœ: æ€»å…±æå– {total_extracted_records} æ¡å¤„ç½šä¿¡æ¯")
            logger.info(f"æ—¶é—´ç»Ÿè®¡: æ€»ç”¨æ—¶ {total_time/60:.2f}åˆ†é’Ÿ, å¹³å‡æ¯æ¡è®°å½• {avg_time_per_record:.2f}ç§’")
            logger.info(f"ä¸´æ—¶æ–‡ä»¶ç›®å½•: {temp_dir}")
            logger.info(f"ä»»åŠ¡å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"="*80)
            
            # å‘é€ç®€åŒ–çš„å®Œæˆä¿¡æ¯
            completion_info = {
                "type": "complete",
                "processed_count": len(df),
                "success_count": success_count,
                "failure_count": failure_count,
                "success_rate": round(success_rate, 2),
                "results": results
            }
            yield f"data: {json.dumps(completion_info, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_info = {
                "type": "error",
                "message": f"æ‰¹é‡æå–ä»»åŠ¡å¤±è´¥: {str(e)}"
            }
            yield f"data: {json.dumps(error_info, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )


@router.post("/get-file-columns")
async def get_file_columns(file: UploadFile = File(...)):
    """è·å–ä¸Šä¼ æ–‡ä»¶çš„åˆ—å"""
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒCSVå’ŒExcelæ–‡ä»¶")
        
        # è¯»å–æ–‡ä»¶
        contents = await file.read()
        
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        else:
            df = pd.read_excel(io.BytesIO(contents))
        
        # è¿”å›åˆ—åå’Œå‰å‡ è¡Œæ•°æ®ä½œä¸ºé¢„è§ˆ
        columns = df.columns.tolist()
        preview_data = df.to_dict('records')
        
        return {
            "columns": columns,
            "preview_data": preview_data,
            "total_rows": len(df)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.post("/batch-classify")
async def batch_classify(
    file: UploadFile = File(...),
    labels: str = Form(...)
):
    """Batch classify cases from uploaded CSV file"""
    try:
        # Validate file type
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="Only CSV files are supported")
        
        # Read CSV file
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))
        
        # Parse labels
        label_list = [label.strip() for label in labels.split(',')]
        
        # Check if required columns exist
        if 'content' not in df.columns and 'text' not in df.columns:
            raise HTTPException(status_code=400, detail="CSV must contain 'content' or 'text' column")
        
        text_column = 'content' if 'content' in df.columns else 'text'
        
        # Classify each row
        results = []
        for index, row in df.iterrows():
            text = str(row[text_column])
            
            # Simple classification logic (same as single text classification)
            text_lower = text.lower()
            predicted_labels = []
            confidence = 0.0
            
            keyword_mapping = {
                "è¿è§„æ”¾è´·": ["æ”¾è´·", "è´·æ¬¾", "ä¿¡è´·"],
                "å†…æ§ç®¡ç†": ["å†…æ§", "ç®¡ç†", "åˆ¶åº¦"],
                "åæ´—é’±": ["æ´—é’±", "åæ´—é’±", "å¯ç–‘äº¤æ˜“"],
                "æ¶ˆè´¹è€…æƒç›Š": ["æ¶ˆè´¹è€…", "æƒç›Š", "æŠ•è¯‰"],
                "ä¿¡æ¯æŠ«éœ²": ["æŠ«éœ²", "ä¿¡æ¯", "å…¬å¼€"],
                "é£é™©ç®¡ç†": ["é£é™©", "ç®¡ç†", "æ§åˆ¶"]
            }
            
            for label in label_list:
                if label in keyword_mapping:
                    keywords = keyword_mapping[label]
                    if any(keyword in text_lower for keyword in keywords):
                        predicted_labels.append(label)
                        confidence += 0.8
            
            if not predicted_labels:
                predicted_labels = [label_list[0]] if label_list else ["å…¶ä»–"]
                confidence = 0.3
            
            confidence = min(confidence / len(predicted_labels), 1.0) if predicted_labels else 0.0
            
            # Add results to original row
            result_row = row.to_dict()
            result_row['predicted_labels'] = ','.join(predicted_labels)
            result_row['confidence'] = round(confidence, 2)
            results.append(result_row)
        
        # Convert results to CSV
        result_df = pd.DataFrame(results)
        csv_buffer = io.StringIO()
        result_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        return {
            "message": "Batch classification completed",
            "processed_count": len(results),
            "csv_data": csv_data
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))